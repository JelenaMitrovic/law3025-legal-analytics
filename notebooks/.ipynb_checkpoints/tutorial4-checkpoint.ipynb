{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Analytics (LAW3025) - Tutorial 4: 'Numerical Exploratory Data Analysis'\n",
    "\n",
    "*Version*: 2021/2022\n",
    "\n",
    "This notebook accompanies Chapter 6: Summarizing Data of [Epstein and Martin's *An Introduction to Empirical Legal Research'*](https://maastrichtuniversity.on.worldcat.org/oclc/891136365), where Epstein and Martin explore the International Criminals Tribunals (ICT) dataset. The dataset 'dataset_ictdata.csv' is already included in the zip-file provided with this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we dealt with data preparation and data understanding. Now, we delve deeper into data understanding with Exploratory Data Analysis. You will do a basic numerical  exploratory data analysis. In the next tutorial you will do a visual exploratory data analysis. \n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is all about getting to know your data. One of the most elementary steps to do this is by getting a basic description of your data. A basic description of your data is indeed a very broad term: you can interpret it as a quick and dirty way to get some information on your data, as a way of getting some simple, easy-to-understand information on your data, to get a basic feel for your data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Data Preparation and Understanding\n",
    "\n",
    "### 2.1. Load the data\n",
    "\n",
    "We are going to use `pandas`, so you will need to load the csv into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a DataFrame named 'data'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Inspect the data\n",
    "\n",
    "In tutorial 3, you learned how to inspect, clean and manipulate data. \n",
    "\n",
    "The `info()` function is used to print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data using the info() function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the `head()` and `tail()` functions of the Pandas library, you can easily check out the first and last lines of your DataFrame, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first five rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the last ten rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Manipulate the data\n",
    "\n",
    "Many columns have numerical values for categorical data (eg `verdict`, `tribunal`, and `crimRank`) and boolean data (1: Yes, 0: No). However, the data type given by the `info()` function does not match it. Also, `lastName` and `firstName` should be strings.\n",
    "\n",
    "We use the [codebook](http://empiricallegalresearch.org/assets/Codebook_ict.pdf) and the `replace()` and `astype()` functions to make the dataset easier to understand and analyse.\n",
    "\n",
    "This also makes our life in the the numerical EDA a bit easier, because the categorical and boolean data are not mistaken for integers or floating points (e.g `numGuil` and `guiltyPlea`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the following:\n",
    "# - Change data type of the strings to 'string' (Hint: use the .astype() function)\n",
    "# - Change data type of the booleans to 'boolean' (Hint: use the .astype() function)\n",
    "# - Change data type of the categorical data to 'category' (Hint: use the .replace() function to change the values of the given column before suffixing the .astype() function. Example: data['column name'] = data['column name'].replace().astype())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check with info() to confirm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the first rows of the manipulated dataset again to confirm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numerical EDA\n",
    "\n",
    "You can use the `describe()` function to get various summary statistics that exclude NaN values. \n",
    "\n",
    "For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a summary table of descriptive statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show several for each column in the dataset. The first number, the `count`, gives the number of non-NA/null observations (rows).\n",
    "\n",
    "For object data, the result’s index will include `count`, `unique`, `top`, and `freq.` The `unique` is the number of of unique values. The `top` is the most common value, also known as the 'mode' in statistics. The `freq` is the most common value’s frequency. Timestamps also include the first and last items.\n",
    "\n",
    "If you want to know what the unique values for the data are, you can always apply the `unique()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values for tribunal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second value in the descriptives table for numerical data is the `mean`, which is the average value for the variable. \n",
    "\n",
    "Next, `std` has nothing to do with your annual routine check up at your local health clinic (seriously, get tested!) but is short for 'standard deviation', which measures how numerically spread out the values are.\n",
    "\n",
    "To interpret the `min`, `25%`, `50%`, `75%` and `max` values, imagine sorting each column from lowest to highest value. The first (minimum/smallest) value is the `min`. If you go a quarter way through the list, you'll find a number that is bigger than 25% of the values and smaller than 75% of the values. That is the 25% value (pronounced \"25th percentile\"). The 75th percentile is defined analogously. The median value is represented by the value for `50%`: half of the data points are below this value and the other half are above this value. The `max` is the maximum or largest value.\n",
    "\n",
    "Note that, of course, there are many packages available in Python that can give you those statistics, including Pandas itself. Using this function is just one of the ways to get this information.\n",
    "\n",
    "Also note that you certainly need to take the time to dive deeper into the descriptive statistics if you haven’t done this yet. You can use these descriptive statistics to begin to assess the quality of your data. Then you’ll be able to decide whether you need to correct, discard or deal with the data in another way. This is usually the data profiling step. This step in the EDA is meant to understand the data elements and its anomalies a bit better and to see how the data matches the documentation on the one hand and accommodates to the research goals (or business needs) on the other hand.\n",
    "\n",
    "Other useful functions:\n",
    "- `DataFrame.count` Count number of non-NA/null observations.\n",
    "- `DataFrame.max` Maximum of the values in the object.\n",
    "- `DataFrame.min` Minimum of the values in the object.\n",
    "- `DataFrame.mean` Mean of the values.\n",
    "- `DataFrame.std` Standard deviation of the observations.\n",
    "\n",
    "The 'Exploratory data analysis' chapter in the DataCamp course [*pandas Foundations*](https://learn.datacamp.com/courses/pandas-foundations) explains these functions in more detail.\n",
    "\n",
    "The `describe()` function is nice to get a first statistical summary of the entire dataset. Sometimes, you only need the summary statistics for one variable. So let's practice that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the summary statistics only for the sentence column\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
